{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "Es toda la información sobre nuestro clasificador para una determinada regla de decisión. La regla impacta mucho sobre el valor final de la métrica, así que las comparaciones tienen que ser justas. Por ejemplo, si tengo una clase mucho más chica que las otras y uso una regla de decisión que consiste en decidir por la clase de mayor probabilidad, la mejora en esa clase no es lo mismo para todas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>neg</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>observed</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neutral</th>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          pos  neg  neutral\n",
       "observed                   \n",
       "pos        15   10      100\n",
       "neg        10   15       10\n",
       "neutral    10  100     1000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def illustrative_confusion_matrix(data):\n",
    "    classes = ['pos', 'neg', 'neutral']\n",
    "    ex = pd.DataFrame(\n",
    "        data,        \n",
    "        columns=classes,\n",
    "        index=classes)\n",
    "    ex.index.name = \"observed\"\n",
    "    return ex\n",
    "\n",
    "ex1 = illustrative_confusion_matrix([\n",
    "    [15,  10,  100],\n",
    "    [10,  15,   10],\n",
    "    [10, 100, 1000]])\n",
    "\n",
    "ex1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El eje x son las muestras predecidas y el eje y son las muestras de verdad. Es decir que el punto (x=neg,y=pos) dice cuántas muestras fueron predecidas como negativas y en realidad eran positivas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Es la cantidad de muestras bien predecidas sobre la cantidad total de predicciones. Es la diagonal de la matriz de confusión sobre la suma de todos los valores de la matriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8110236220472441"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(cm):\n",
    "    return cm.values.diagonal().sum() / cm.values.sum()\n",
    "\n",
    "accuracy(ex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puedo tener buena accuracy pero sólo por el hecho de que el dataset es hamable con mi regla de decisión. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8740157480314961\n"
     ]
    }
   ],
   "source": [
    "ex2 = illustrative_confusion_matrix([\n",
    "    [0, 0,  125],\n",
    "    [0, 0,   35], \n",
    "    [0, 0, 1110]])\n",
    "\n",
    "print(accuracy(ex2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y sin embargo se observa el clasificador lo único que hace es asignar todas las muestras a la clase 3.\n",
    "\n",
    "La función de costo Cross-Entropy entre la clase verdadera y la probabilidad estimada se define como\n",
    "\n",
    "$$\n",
    "-\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{k=1}^{K} y_{i,k} \\log(p_{i,k})\n",
    "$$\n",
    "\n",
    "por lo que es equivalente, para el caso en que $y_i$ es un *one-hot-vector*, a maximizar el score de la clase ganadora en cada muestra. Es decir, se está maximizando la accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "Es el \"accuracy por clase\". Es decir, para cada clase se calcula la proporción de aciertos sobre el total de muestras que pertenecían a esa clase.\n",
    "\n",
    "En términos de la matriz de confusión es la diagonal dividido la suma por culumna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos        0.428571\n",
       "neg        0.120000\n",
       "neutral    0.900901\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def precision(cm):\n",
    "    return cm.values.diagonal() / cm.sum(axis=0)\n",
    "\n",
    "precision(ex1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay que tener cuidado con la precision cuando es igual a cero o a uno:\n",
    "\n",
    "* Si es igual a 1 (o muy muy cerca de 1) puede ser que igual tenga muchas muestras en esa clase que en realidad pertenecían a otra (me tengo que fijar el recall)\n",
    "\n",
    "* Si es cero, puede ser que sea realmente muy baja precisión pero también puede ser un mapeo que viene de una NaN, originado probablemente porque no se clasificó ninguna muestra como esa clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall\n",
    "\n",
    "Es un complemento para la precision. Te dice cómo le fue a cada clase en relación con las verdaderas predicciones. En términos de la matriz de confusión es la cantidad de aciertos por clase dividido la suma de las filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(cm):\n",
    "    return cm.values.diagonal() / cm.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
