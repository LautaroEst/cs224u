{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lestien/Documents/Cursos/cs224u - Natural Language Understanding/assignments/assignment 2/BecaNLP/Utils/Datasets/Stanford Sentiment Treebank\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(sys.path[0].split('Cursos')[0],'BecaNLP/Utils'))\n",
    "\n",
    "import NLPUtils as nlp\n",
    "from NLPUtils.datasets import sst\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8544\n",
      "6920\n",
      "8544\n"
     ]
    }
   ],
   "source": [
    "labels = [label for tree, label in sst.train_reader()]\n",
    "print(len(labels))\n",
    "labels = [label for tree, label in sst.train_reader(class_func=sst.binary_class_func)]\n",
    "print(len(labels))\n",
    "labels = [label for tree, label in sst.train_reader(class_func=sst.ternary_class_func)]\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3610\n",
       "0    3310\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label for tree, label in sst.train_reader(class_func=sst.binary_class_func)]\n",
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'hola': 2, 'esto': 2, 'es': 2, 'una': 2, 'prueba': 2, 'también': 1})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "corpus = [['hola', 'esto', 'es', 'una', 'prueba'],\n",
    "         ['hola', 'esto', 'también','es', 'una', 'prueba']]\n",
    "Counter(chain.from_iterable(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NLPUtils.datasets import sst\n",
    "\n",
    "#tk_to_freq = sst.get_full_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sst.get_bow_dataset(None, data=['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18278])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18278"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['vocab'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"''\": 0, \"'d\": 1, \"'s\": 2, ',': 3, '--': 4, '.': 5, '21st': 6, 'Adams': 7, 'America': 8, 'Arnold': 9, 'British': 10, 'Bryan': 11, 'Century': 12, 'Conan': 13, 'Damme': 14, 'J.R.R.': 15, 'Jackson': 16, 'Jean-Claud': 17, 'Lord': 18, 'Middle-earth': 19, 'Peter': 20, 'Rings': 21, 'Rock': 22, 'Schwarzenegger': 23, 'Segal': 24, 'Singer\\\\/composer': 25, 'Steven': 26, 'The': 27, 'Tolkien': 28, 'Van': 29, 'You': 30, '``': 31, 'a': 32, 'adequately': 33, 'and': 34, 'be': 35, 'but': 36, 'by': 37, 'can': 38, 'captures': 39, 'certainly': 40, 'co-writer\\\\/director': 41, 'column': 42, 'continuation': 43, 'contributes': 44, 'describe': 45, 'destined': 46, 'eccentrics': 47, 'elaborate': 48, 'enough': 49, 'er': 50, 'even': 51, 'expanded': 52, 'few': 53, 'going': 54, 'gold': 55, 'gorgeously': 56, 'greater': 57, 'had': 58, 'have': 59, 'he': 60, 'hearts': 61, 'hits': 62, 'huge': 63, 'intended': 64, 'intrusive': 65, 'is': 66, 'make': 67, 'more': 68, 'new': 69, 'not': 70, 'now': 71, 'of': 72, 'or': 73, 'package': 74, 'piece': 75, 'plucky': 76, 'potential': 77, 'simply': 78, 'slew': 79, 'so': 80, 'songs': 81, 'spirit': 82, 'splash': 83, 'story': 84, 'than': 85, 'that': 86, 'the': 87, 'think': 88, 'to': 89, 'trilogy': 90, 'vision': 91, 'whole': 92, 'with': 93, 'words': 94, 'would': 95}\n",
      "[\"''\", \"'d\", \"'s\", ',', '--', '.', '21st', 'Adams', 'America', 'Arnold', 'British', 'Bryan', 'Century', 'Conan', 'Damme', 'J.R.R.', 'Jackson', 'Jean-Claud', 'Lord', 'Middle-earth', 'Peter', 'Rings', 'Rock', 'Schwarzenegger', 'Segal', 'Singer\\\\/composer', 'Steven', 'The', 'Tolkien', 'Van', 'You', '``', 'a', 'adequately', 'and', 'be', 'but', 'by', 'can', 'captures', 'certainly', 'co-writer\\\\/director', 'column', 'continuation', 'contributes', 'describe', 'destined', 'eccentrics', 'elaborate', 'enough', 'er', 'even', 'expanded', 'few', 'going', 'gold', 'gorgeously', 'greater', 'had', 'have', 'he', 'hearts', 'hits', 'huge', 'intended', 'intrusive', 'is', 'make', 'more', 'new', 'not', 'now', 'of', 'or', 'package', 'piece', 'plucky', 'potential', 'simply', 'slew', 'so', 'songs', 'spirit', 'splash', 'story', 'than', 'that', 'the', 'think', 'to', 'trilogy', 'vision', 'whole', 'with', 'words', 'would']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "\n",
    "data = []\n",
    "for i, (tree, label) in enumerate(sst.train_reader()):\n",
    "    if i > 3:\n",
    "        break\n",
    "    data.append(Counter(tree.leaves()))\n",
    "\n",
    "vectorizer.fit(data)\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sst.get_bow_dataset(data=['train'],vocab=None, n_gram=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.'] len = 36\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float64) len = 18278\n",
      "18278\n",
      "tensor(34)\n",
      "Counter({'to': 2, \"'s\": 2, 'The': 1, 'Rock': 1, 'is': 1, 'destined': 1, 'be': 1, 'the': 1, '21st': 1, 'Century': 1, 'new': 1, '``': 1, 'Conan': 1, \"''\": 1, 'and': 1, 'that': 1, 'he': 1, 'going': 1, 'make': 1, 'a': 1, 'splash': 1, 'even': 1, 'greater': 1, 'than': 1, 'Arnold': 1, 'Schwarzenegger': 1, ',': 1, 'Jean-Claud': 1, 'Van': 1, 'Damme': 1, 'or': 1, 'Steven': 1, 'Segal': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "comment = next(sst.train_reader())[0].leaves()\n",
    "print(comment, 'len =',len(comment))\n",
    "x = dataset['train'][0][0]\n",
    "print(x,'len =',len(x))\n",
    "print(len(dataset['vocab']))\n",
    "for tk in comment:\n",
    "    try:\n",
    "        dataset['vocab'][tk]\n",
    "    except KeyError:\n",
    "        print(tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century', \"'s\", 'new', '``', 'Conan', \"''\", 'and', 'that', 'he', \"'s\", 'going', 'to', 'make', 'a', 'splash', 'even', 'greater', 'than', 'Arnold', 'Schwarzenegger', ',', 'Jean-Claud', 'Van', 'Damme', 'or', 'Steven', 'Segal', '.']\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "3495\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(2., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(2., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(2., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(2., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "tensor(1., dtype=torch.float64)\n",
      "36\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(dataset['train'][0][0])\n",
    "print(dataset['vocab']['Rock'])\n",
    "tk_to_idx = {tk: idx for idx, tk in enumerate(dataset['vocab'].keys())}\n",
    "\n",
    "indeces = []\n",
    "for tk in next(sst.train_reader())[0].leaves():\n",
    "    idx = tk_to_idx[tk]\n",
    "    print(dataset['train'][0][0][0,idx])\n",
    "    indeces.append(idx)\n",
    "    \n",
    "print(len(indeces))\n",
    "print(len(dataset['train'][0][0][0,dataset['train'][0][0][0,:]!=0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (comment, label) in enumerate(sst.train_reader()):\n",
    "    if dataset['train'][i][1] != int(label):\n",
    "        print('Nop')\n",
    "    if dataset['train'][i][0].sum() != len(comment.leaves()):\n",
    "        print('Nones')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sst.get_bow_dataset(data=['dev'],vocab=None, n_gram=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'lovely': 2, 'It': 1, \"'s\": 1, 'a': 1, 'film': 1, 'with': 1, 'performances': 1, 'by': 1, 'Buy': 1, 'and': 1, 'Accorsi': 1, '.': 1})\n",
      "{'Accorsi'}\n",
      "Counter({'No': 1, 'one': 1, 'goes': 1, 'unindicted': 1, 'here': 1, ',': 1, 'which': 1, 'is': 1, 'probably': 1, 'for': 1, 'the': 1, 'best': 1, '.': 1})\n",
      "{'unindicted'}\n",
      "Counter({'and': 2, ',': 2, 'Uses': 1, 'sharp': 1, 'humor': 1, 'insight': 1, 'into': 1, 'human': 1, 'nature': 1, 'to': 1, 'examine': 1, 'class': 1, 'conflict': 1, 'adolescent': 1, 'yearning': 1, 'the': 1, 'roots': 1, 'of': 1, 'friendship': 1, 'sexual': 1, 'identity': 1, '.': 1})\n",
      "{'Uses'}\n",
      "Counter({'Half': 2, ',': 2, 'Submarine': 1, 'flick': 1, 'Ghost': 1, 'Story': 1, 'All': 1, 'in': 1, 'one': 1, 'criminally': 1, 'neglected': 1, 'film': 1})\n",
      "{'criminally', 'Submarine'}\n",
      "Counter({'its': 2, 'characters': 2, 'and': 2, 'Dazzles': 1, 'with': 1, 'fully-written': 1, ',': 1, 'determined': 1, 'stylishness': 1, '-LRB-': 1, 'which': 1, 'always': 1, 'relates': 1, 'to': 1, 'story': 1, '-RRB-': 1, 'Johnny': 1, 'Dankworth': 1, \"'s\": 1, 'best': 1, 'soundtrack': 1, 'in': 1, 'years': 1, '.': 1})\n",
      "{'fully-written', 'relates', 'stylishness', 'Dankworth', 'Dazzles'}\n",
      "Counter({',': 2, 'a': 2, 'Visually': 1, 'imaginative': 1, 'thematically': 1, 'instructive': 1, 'and': 1, 'thoroughly': 1, 'delightful': 1, 'it': 1, 'takes': 1, 'us': 1, 'on': 1, 'roller-coaster': 1, 'ride': 1, 'from': 1, 'innocence': 1, 'to': 1, 'experience': 1, 'without': 1, 'even': 1, 'hint': 1, 'of': 1, 'that': 1, 'typical': 1, 'kiddie-flick': 1, 'sentimentality': 1, '.': 1})\n",
      "{'kiddie-flick'}\n",
      "Counter({'the': 2, 'of': 2, ',': 2, 'character': 2, '--': 2, 'Unlike': 1, 'speedy': 1, 'wham-bam': 1, 'effect': 1, 'most': 1, 'Hollywood': 1, 'offerings': 1, 'development': 1, 'and': 1, 'more': 1, 'importantly': 1, 'empathy': 1, 'is': 1, 'at': 1, 'heart': 1, 'Italian': 1, 'for': 1, 'Beginners': 1, '.': 1})\n",
      "{'wham-bam'}\n",
      "Counter({'and': 2, ',': 2, 'a': 2, 'You': 1, \"'ll\": 1, 'gasp': 1, 'appalled': 1, 'laugh': 1, 'outraged': 1, 'possibly': 1, 'watching': 1, 'the': 1, 'spectacle': 1, 'of': 1, 'promising': 1, 'young': 1, 'lad': 1, 'treading': 1, 'desperately': 1, 'in': 1, 'nasty': 1, 'sea': 1, 'shed': 1, 'an': 1, 'errant': 1, 'tear': 1, '.': 1})\n",
      "{'appalled', 'outraged', 'shed', 'errant'}\n",
      "Counter({'The': 1, 'band': 1, \"'s\": 1, 'courage': 1, 'in': 1, 'the': 1, 'face': 1, 'of': 1, 'official': 1, 'repression': 1, 'is': 1, 'inspiring': 1, ',': 1, 'especially': 1, 'for': 1, 'aging': 1, 'hippies': 1, '-LRB-': 1, 'this': 1, 'one': 1, 'included': 1, '-RRB-': 1, '.': 1})\n",
      "{'hippies', 'repression', 'official'}\n",
      "Counter({'Although': 1, 'German': 1, 'cooking': 1, 'does': 1, 'not': 1, 'come': 1, 'readily': 1, 'to': 1, 'mind': 1, 'when': 1, 'considering': 1, 'the': 1, 'world': 1, \"'s\": 1, 'best': 1, 'cuisine': 1, ',': 1, 'Mostly': 1, 'Martha': 1, 'could': 1, 'make': 1, 'Deutchland': 1, 'a': 1, 'popular': 1, 'destination': 1, 'for': 1, 'hungry': 1, 'tourists': 1, '.': 1})\n",
      "{'Deutchland', 'cooking'}\n",
      "Counter({'A': 1, 'beguiling': 1, 'splash': 1, 'of': 1, 'pastel': 1, 'colors': 1, 'and': 1, 'prankish': 1, 'comedy': 1, 'from': 1, 'Disney': 1, '.': 1})\n",
      "{'prankish'}\n",
      "Counter({'as': 5, 'a': 2, 'As': 1, 'surreal': 1, 'dream': 1, 'and': 1, 'detailed': 1, 'photograph': 1, ',': 1, 'visually': 1, 'dexterous': 1, 'it': 1, 'is': 1, 'at': 1, 'times': 1, 'imaginatively': 1, 'overwhelming': 1, '.': 1})\n",
      "{'photograph', 'dexterous'}\n",
      "Counter({',': 4, 'the': 3, '-LRB-': 1, 'Lawrence': 1, 'bounces': 1, '-RRB-': 1, 'all': 1, 'over': 1, 'stage': 1, 'dancing': 1, 'running': 1, 'sweating': 1, 'mopping': 1, 'his': 1, 'face': 1, 'and': 1, 'generally': 1, 'displaying': 1, 'wacky': 1, 'talent': 1, 'that': 1, 'brought': 1, 'him': 1, 'fame': 1, 'in': 1, 'first': 1, 'place': 1, '.': 1})\n",
      "{'displaying'}\n",
      "Counter({'The': 1, 'film': 1, 'serves': 1, 'as': 1, 'a': 1, 'valuable': 1, 'time': 1, 'capsule': 1, 'to': 1, 'remind': 1, 'us': 1, 'of': 1, 'the': 1, 'devastating': 1, 'horror': 1, 'suffered': 1, 'by': 1, 'an': 1, 'entire': 1, 'people': 1, '.': 1})\n",
      "{'capsule'}\n",
      "Counter({'the': 2, 'What': 1, \"'s\": 1, 'surprising': 1, 'about': 1, 'Full': 1, 'Frontal': 1, 'is': 1, 'that': 1, 'despite': 1, 'its': 1, 'overt': 1, 'self-awareness': 1, ',': 1, 'parts': 1, 'of': 1, 'movie': 1, 'still': 1, 'manage': 1, 'to': 1, 'break': 1, 'past': 1, 'artifice': 1, 'and': 1, 'thoroughly': 1, 'engage': 1, 'you': 1, '.': 1})\n",
      "{'overt'}\n",
      "Counter({'...': 1, 'an': 1, 'otherwise': 1, 'intense': 1, ',': 1, 'twist-and-turn': 1, 'thriller': 1, 'that': 1, 'certainly': 1, 'should': 1, \"n't\": 1, 'hurt': 1, 'talented': 1, 'young': 1, 'Gaghan': 1, \"'s\": 1, 'resume': 1, '.': 1})\n",
      "{'twist-and-turn'}\n",
      "Counter({',': 2, 'and': 2, 'the': 2, '--': 2, 'It': 1, 'moves': 1, 'quickly': 1, 'adroitly': 1, 'without': 1, 'fuss': 1, ';': 1, 'it': 1, 'does': 1, \"n't\": 1, 'give': 1, 'you': 1, 'time': 1, 'to': 1, 'reflect': 1, 'on': 1, 'inanity': 1, 'Cold': 1, 'War': 1, 'datedness': 1, 'of': 1, 'its': 1, 'premise': 1, '.': 1})\n",
      "{'inanity', 'datedness', 'adroitly'}\n",
      "Counter({'the': 3, 'film': 2, '--': 2, 'The': 1, \"'s\": 1, 'welcome': 1, 'breeziness': 1, 'and': 1, 'some': 1, 'unbelievably': 1, 'hilarious': 1, 'moments': 1, 'most': 1, 'portraying': 1, 'idiocy': 1, 'of': 1, 'industry': 1, 'make': 1, 'it': 1, 'mostly': 1, 'worth': 1, 'trip': 1, '.': 1})\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-2ed098b1d49c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mindeces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0munk_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0midx_to_tk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindeces\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munk_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-110-2ed098b1d49c>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mindeces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dev'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0munk_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0midx_to_tk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindeces\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munk_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "idx_to_tk = {idx: tk for idx, tk in enumerate(dataset['vocab'].keys())}\n",
    "\n",
    "for i, (comment, label) in enumerate(sst.dev_reader()):\n",
    "    if dataset['dev'][i][1] != int(label):\n",
    "        print('Nop')\n",
    "    if dataset['dev'][i][0].sum() != len(comment.leaves()):\n",
    "        counts = Counter(comment.leaves())\n",
    "        print(counts)\n",
    "        indeces = [i for i, idx in enumerate(dataset['dev'][i][0]) if idx!= 0]\n",
    "        unk_words = set(counts.keys()) - {idx_to_tk[idx] for idx in indeces}\n",
    "        print(unk_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
